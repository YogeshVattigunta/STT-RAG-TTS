# Local Voice Agent with 3D Avatar (Offline-Capable)

A fully local **voice-based AI assistant** featuring real-time speech interaction, retrieval-augmented generation (RAG), text-to-speech (TTS), and an animated 3D avatar UI.

Built to be **offline-first**, **demo-ready**, and **interview-ready**.

---

## Features

-  **Speech-to-Text (STT)**  
  Converts live microphone input into text.

- **Retrieval-Augmented Generation (RAG)**  
  Answers are grounded in your own knowledge base (`docs.txt`).

-  **Local LLM**  
  Uses a locally running language model (via Ollama or equivalent).

-  **Text-to-Speech (TTS)**  
  Speaks responses aloud using a local TTS engine.

-  **Animated 3D Avatar**  
  GLB model with embedded animation (Idle Animation).

- ğŸ **State-Synced Animation**  
  Avatar animation speed reflects system state:
  - Listening
  - Thinking
  - Speaking

-  **Modern UI**
  - Space-themed background
  - Two-panel layout (controls on left, avatar on right)
  - Clean, readable conversation panel

-  **Offline Ready**
  - Works without internet after models are downloaded.

---

##  High-Level Architecture

User Voice
â†“
Speech-to-Text (STT)
â†“
User Query
â†“
RAG (FAISS + Sentence Embeddings)
â†“
Local LLM
â†“
Response Text
â†“
Text-to-Speech (TTS)
â†“
Audio Output


The frontend **polls backend status** to:
- update UI text
- trigger avatar animation changes
- reflect current system state

---

##  Folder Structure

project-root/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                # FastAPI backend server
â”‚   â”œâ”€â”€ voice_core.py         # Voice pipeline (STT â†’ RAG â†’ LLM â†’ TTS)
â”‚   â”œâ”€â”€ docs.txt              # Knowledge base for RAG
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ audio/
â”‚       â”œâ”€â”€ input.wav         # Recorded user audio
â”‚       â””â”€â”€ output.wav        # Generated speech audio
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html            # Main UI layout
â”‚   â”œâ”€â”€ style.css             # Styling + space background
â”‚   â”œâ”€â”€ app.js                # UI logic + backend polling
â”‚   â”œâ”€â”€ robot.glb             # Animated 3D avatar (GLB)
â”‚   â””â”€â”€ space_bg.png          # Space background image
â”‚
â””â”€â”€ README.md                 # Project documentation


---

##  Tech Stack

### Backend
- **Python 3.10+**
- **FastAPI** â€“ API server
- **Whisper / faster-whisper** â€“ Speech-to-text
- **FAISS** â€“ Vector search for RAG
- **Sentence-Transformers** â€“ Text embeddings
- **Local LLM** (e.g., via Ollama)
- **TTS Engine** â€“ Local text-to-speech

### Frontend
- **HTML / CSS / JavaScript**
- **`<model-viewer>`** â€“ 3D avatar rendering
- **GLB model** with embedded animation
- **Polling-based UI updates**

---

##  Knowledge Base (RAG)

The file `backend/docs.txt` acts as the assistantâ€™s **knowledge source**.

Example:
```txt
The user's name is Yogesh.
Yogesh prefers simple explanations.
Transformers use self-attention to process sequences.
RAG improves accuracy by retrieving relevant context.

 How to Run
1ï¸) Start the Local LLM

Make sure your local LLM server is running (example using Ollama):

ollama list


(Optional test)

ollama run phi

2ï¸) Start Backend
cd backend
uvicorn app:app --port 8000


Verify:

http://localhost:8000/status

3ï¸) Start Frontend
cd frontend
python -m http.server 5500


Open in browser:

http://localhost:5500

How to Use

Click ğŸ¤ Start Speaking

Speak clearly into the microphone

Click â¹ Stop

Observe:

Status transitions (listening â†’ thinking â†’ speaking)

Text appears in the conversation panel

Avatar animation reacts

Voice response is spoken

System States
State	Description
idle	Waiting for user input
listening	Recording microphone input
thinking	Processing STT + RAG + LLM
speaking	Playing TTS response

Avatar animation speed changes based on these states.

 UI Design Notes

Left panel: interaction & conversation

Right panel: animated avatar

Background: 2D space image (visual mood)

Lighting: neutral 3D environment for realism

Glassmorphism: subtle blur for readability

 Interview-Ready Explanation

â€œThis project implements a fully local voice agent using a modular pipeline for STT, RAG, LLM, and TTS. The frontend remains responsive by polling backend state, while the 3D avatar reacts visually to system states for better user feedback.â€

 Possible Enhancements

Faster STT with GPU or quantized models

Lip-sync or mouth animation

Streaming LLM responses

Waveform visualization during speech

Mobile-responsive layout

Desktop packaging (Electron)

 License

This project is for educational, demo, and research purposes.
You are free to extend or adapt it as needed.




